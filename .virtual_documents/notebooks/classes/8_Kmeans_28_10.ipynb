import os
from pathlib import Path

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from matplotlib.colors import LogNorm
from sklearn import metrics
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

plt.style.use(["seaborn-v0_8-darkgrid"])
plt.rcParams["figure.figsize"] = (14, 7)
plt.rcParams["font.family"] = "DejaVu Sans"

data_path = Path.cwd().parents[1] / "data"


def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], "k.", markersize=2)


def plot_centroids(centroids, weights=None, circle_color="w", cross_color="k"):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(
        centroids[:, 0],
        centroids[:, 1],
        marker="o",
        s=30,
        linewidths=8,
        color=circle_color,
        zorder=10,
        alpha=0.9,
    )
    plt.scatter(
        centroids[:, 0],
        centroids[:, 1],
        marker="x",
        s=50,
        linewidths=50,
        color=cross_color,
        zorder=11,
        alpha=1,
    )


def plot_decision_boundaries(
    clusterer,
    X,
    resolution=1000,
    show_centroids=True,
    show_xlabels=True,
    show_ylabels=True,
):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(
        np.linspace(mins[0], maxs[0], resolution),
        np.linspace(mins[1], maxs[1], resolution),
    )
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Filled Colors
    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap="Pastel2")
    # Contour line
    plt.contour(
        Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors="k"
    )
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)


def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(
        np.linspace(mins[0], maxs[0], resolution),
        np.linspace(mins[1], maxs[1], resolution),
    )
    # Calculate -log densities
    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(
        xx,
        yy,
        Z,
        norm=LogNorm(vmin=1.0, vmax=30.0),  # normalize using log normal
        levels=np.logspace(0, 2, 12),
    )  # 12 levels
    plt.contour(
        xx,
        yy,
        Z,
        norm=LogNorm(vmin=1.0, vmax=30.0),
        levels=np.logspace(0, 2, 12),
        linewidths=1,
        colors="k",
    )

    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])  # get cluster
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z, linewidths=2, colors="r", linestyles="dashed")

    plt.plot(X[:, 0], X[:, 1], "k.", markersize=2)
    plot_centroids(clusterer.means_, clusterer.weights_)

    plt.xlabel("$x_1$", fontsize=14)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)


X = pd.read_csv(data_path / "samsung_train_data.zip", compression="zip")
print(X.shape)
X.head()


y = np.loadtxt(data_path / "samsung_train_labels.txt").astype(int)

X = X.values


X.shape


np.unique(y)


n_classes = np.unique(y).size


plt.figure(figsize=(14, 6))

plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap="coolwarm")

plt.title("Scatterplot of two first features (X1 and X2)")
plt.xlabel("X1")
plt.ylabel("X2")

plt.show()


scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)


pca = PCA()
pca.fit(X_scaled)

X_pca = pca.transform(X_scaled)


X_pca.shape


plt.figure(figsize=(14, 6))

plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap="coolwarm")

plt.title("Scatterplot of two first features (X1 and X2)")
plt.xlabel("X1")
plt.ylabel("X2")

plt.show()


plt.figure(figsize=(14, 6))

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, s=20, cmap="coolwarm")

plt.title("Projection of 561's dimensional space on two first PCs")
plt.xlabel("Principal Component #1")
plt.ylabel("Principal Component #2")

plt.show()


X_two_dim = X_pca[:, :2]

idx = (X_two_dim[:, 0] < 50) & (X_two_dim[:, 1] < 15)

X_two_dim = X_two_dim  # [idx,:]


import warnings

warnings.filterwarnings("ignore")


from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)
X_tsne = tsne.fit_transform(X_scaled)

X_tsne.shape


plt.figure(figsize=(14, 6))

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, s=20, cmap="coolwarm")

plt.title("Projection of 561's dimensional space using t-SNE")
plt.xlabel("t-SNE Component #1")
plt.ylabel("t-SNE Component #2")

plt.show()


import umap

reducer = umap.UMAP(random_state=42)
X_umap = reducer.fit_transform(X_scaled)

X_umap.shape


plt.figure(figsize=(14, 6))

plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, s=20, cmap="coolwarm")

plt.title("Projection of 561's dimensional space using UMAP")
plt.xlabel("UMAP Component #1")
plt.ylabel("UMAP Component #2")

plt.show()


kmeans = KMeans(n_clusters=2)

kmeans.fit(X_two_dim)


kmeans.labels_


plt.figure(figsize=(14, 6))

plt.scatter(X_two_dim[:, 0], X_two_dim[:, 1], c=kmeans.labels_, s=20, cmap="coolwarm")

plt.title("Projection of 561's dimensional space on two first PCs")
plt.xlabel("Principal Component #1")
plt.ylabel("Principal Component #2")

plt.show()


plt.figure(figsize=(14, 6))
plot_decision_boundaries(kmeans, X_two_dim)

plt.title("Centroids and decision boundaries of k-means")

plt.show()


X_two_dim[:1, :]


kmeans.predict([[-16, 2]])


a = kmeans.transform(X_two_dim[:1, :])

print(a)


np.argmin(a)


k_max = 10

inertia = []
for k in range(1, k_max):
    kmeans = KMeans(n_clusters=k).fit(X_two_dim)
    inertia.append(np.sqrt(kmeans.inertia_))


plt.figure(figsize=(12, 4))
plt.plot(range(1, k_max), inertia, marker="s")

plt.title("The Elbow Method using Inertia")
plt.xlabel("Number of clusters k")
plt.ylabel("Inertia")

plt.show()


d = {}
for k in range(2, k_max - 1):
    i = k - 1
    d[k] = (inertia[i] - inertia[i + 1]) / (inertia[i - 1] - inertia[i])


plt.figure(figsize=(12, 4))
plt.plot(range(2, k_max - 1), [x for x in d.values()], marker="s")

plt.title("The Elbow Method using Inertia")
plt.xlabel("Number of clusters k")
plt.ylabel("Comparative decrease in inertia")

plt.show()


kmeans = KMeans(n_clusters=2)

kmeans.fit(X_two_dim)


plt.figure(figsize=(14, 6))

plt.scatter(X_two_dim[:, 0], X_two_dim[:, 1], c=kmeans.labels_, s=20, cmap="coolwarm")

plt.title("Projection of 561's dimensional space on two first PCs")
plt.xlabel("Principal Component #1")
plt.ylabel("Principal Component #2")

plt.show()


plt.figure(figsize=(14, 6))
plot_decision_boundaries(kmeans, X_two_dim)

plt.title("Centroids and decision boundaries of k-means")

plt.show()


explained_variance_ratio = pca.explained_variance_ratio_

cumsum = np.cumsum(explained_variance_ratio)
d = np.argmax(cumsum >= 0.95) + 1
d


plt.figure(figsize=(12, 4))
plt.plot(cumsum)
plt.plot([d, d], [cumsum.min(), 1])
plt.xlabel("Components")
plt.ylabel("% explained variance")
plt.show()


X_95 = X_pca[:, :d]

inertia = []
for k in range(1, k_max):
    kmeans = KMeans(n_clusters=k).fit(X_95)
    inertia.append(np.sqrt(kmeans.inertia_))


plt.figure(figsize=(12, 4))
plt.plot(range(1, k_max), inertia, marker="s")

plt.title("The Elbow Method using Inertia")
plt.xlabel("Number of clusters k")
plt.ylabel("Inertia")

plt.show()


d = {}
for k in range(2, k_max - 1):
    i = k - 1
    d[k] = (inertia[i] - inertia[i + 1]) / (inertia[i - 1] - inertia[i])


plt.figure(figsize=(12, 4))
plt.plot(range(2, k_max - 1), [x for x in d.values()], marker="s")

plt.title("The Elbow Method using Inertia")
plt.xlabel("Number of clusters k")
plt.ylabel("Comparative decrease in inertia")

plt.show()


kmeans = KMeans(n_clusters=2)

kmeans.fit(X_two_dim)


plt.figure(figsize=(14, 6))

plt.scatter(X_95[:, 0], X_95[:, 1], c=kmeans.labels_, s=20, cmap="coolwarm")

plt.title("Projection of 561's dimensional space using PCs")
plt.xlabel("Principal Component #1")
plt.ylabel("Principal Component #2")

plt.show()
